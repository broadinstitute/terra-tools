{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>Update TCGA Workspace Data Model with Compact DRS URLs/Identifier</center>\n",
    "\n",
    "##### Description\n",
    "    This notebook allows a user to update single entity (ex. participant, sample, and pair) data model tables from a  \"drs://dataguids.org\" file path, denoting the location of a file, to the newer compact DRS URL - drs://dg.4DFC:UUID - format. The notebook will isolate eligible columns that point to a file location with the dataguids.org pointer, create a new updated tsv file, and update the data model.\n",
    "\n",
    "\n",
    "##### Options\n",
    "    The dry_run option (default = True) will print out the changes that will be made to each table ahead of modifying the data tables. The stdout will show the data table name, the individual columns, and the path to the workspace bucket location of the updated .tsv. Users can examine and verify the changes before setting dry_run = False and re-running the cells to make real updates to the data model table.\n",
    "    \n",
    "\n",
    "##### Execution\n",
    "    1. Set dry_run = True or dry_run = False and execute cell (Shift + Enter).\n",
    "    2. Run each following cell once the preeceding cell has completed.\n",
    "        [*] denotes a cell that is not finished executing.\n",
    "\n",
    "##### Notes\n",
    "    The set entity (participant_set, sample_set, and pair_set) data model tables are not modified in this script. The set tables point to the unique IDs of the set constituents - a value that is not modified - thus, not requiring any updates.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# variable that allows user to run script and look at the updated .tsv files before updating data model\n",
    "# set dry_run to \"False\" and re-run script to perform actual update of data model with DRS URLs\n",
    "\n",
    "# DEFAULT: dry_run is set to True and will list the columns in each table that will be updated\n",
    "#          it will also provide the location of the .tsv files with the DRS url updates for inspection\n",
    "\n",
    "dry_run = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Imports relevant packages. (Shift + Enter) to execute.\n",
    "\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from firecloud import api as fapi\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import csv\n",
    "import pprint\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Sets up workspace environment variables. (Shift + Enter) to execute.\n",
    "\n",
    "ws_project = os.environ['WORKSPACE_NAMESPACE']\n",
    "ws_name = os.environ['WORKSPACE_NAME']\n",
    "ws_bucket = os.environ['WORKSPACE_BUCKET']\n",
    "\n",
    "# print(ws_project + \"\\n\" + ws_name + \"\\n\" + \"bucket: \" + ws_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Gets list of single entity types in workspace that need DRS URL updates. (Shift + Enter) to execute.\n",
    "    \n",
    "# API call to get all entity types in workspace\n",
    "res_etypes = fapi.list_entity_types(ws_project, ws_name)\n",
    "dict_all_etypes = json.loads(res_etypes.text)\n",
    "\n",
    "# get non-set entities and add to list\n",
    "# \"set\" entities do not need to be updated because they only reference the unique ID of each single entity\n",
    "# the unique ID of any single entity is not modified so sets should remain the same\n",
    "single_etypes_list = []\n",
    "single_etypes_list = [key for key in dict_all_etypes.keys() if not key.endswith(\"_set\")]\n",
    "\n",
    "print(f\"List of entity types that will be updated, if applicable:\")\n",
    "print('\\n'.join(['\\t' * 7 + c for c in single_etypes_list]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Updates the data model, for single entity types, with DRS URLs. (Shift + Enter) to execute.\n",
    "\n",
    "# set guid pattern for guid validation\n",
    "guid_pattern = re.compile(r'^[\\da-f]{8}-([\\da-f]{4}-){3}[\\da-f]{12}$', re.IGNORECASE)\n",
    "\n",
    "for etype in single_etypes_list:\n",
    "    print(f'Starting TCGA DRS updates for entity: {etype}')\n",
    "    \n",
    "    # get entity table response for API call\n",
    "    res_etype = fapi.get_entities_tsv(ws_project, ws_name, etype, model=\"flexible\")\n",
    "    \n",
    "    # Save current/original data model tsv files to the bucket for provenance\n",
    "    print(f'Saving original {etype} TSV to {ws_bucket}')\n",
    "    df = pd.read_csv(StringIO(res_etype.text), sep=\"\\t\")\n",
    "    original_tsv_name = \"original_\" + etype + \"_table.tsv\"\n",
    "    # write data frames to .tsv files\n",
    "    df.to_csv(original_tsv_name, sep=\"\\t\", index=False)\n",
    "    !gsutil cp $original_tsv_name $ws_bucket 2> stdout\n",
    "    \n",
    "    # read entity table response into dictionary to perform DRS URL updates \n",
    "    dict_etype = list(csv.DictReader(StringIO(res_etype.text), delimiter='\\t'))\n",
    "\n",
    "    # create empty list to add updated rows and list to capture list of columns that were modified\n",
    "    drs_dict_table = []\n",
    "    modified_cols = []\n",
    "    # for \"row\" (each row is [list] of column:values)\n",
    "    for row in dict_etype:\n",
    "        drs_row = row.copy()      \n",
    "        # for each column in row\n",
    "        for col in row:\n",
    "            # check if the col values are dataguids.org URLs and parse out guid\n",
    "            if row[col].startswith(\"drs://dataguids.org\"):\n",
    "                guid = row[col].split(\"/\")[3]  #[0]\n",
    "                # only modify col if guid is valid and exists\n",
    "                if guid and guid_pattern.match(guid):\n",
    "                    drs_url = \"drs://dg.4DFC:\" + guid\n",
    "                    drs_row[col] = drs_url\n",
    "                    modified_cols.append(col)\n",
    "                else:\n",
    "                    None\n",
    "\n",
    "        # append new \"row\" with updated drs values to new list\n",
    "        drs_dict_table.append(drs_row)\n",
    "        \n",
    "        # set output file name and write tsv files\n",
    "        updated_tsv_name = \"updated_\" + etype + \"_table.tsv\"\n",
    "        tsv_headers = drs_dict_table[0].keys()\n",
    "        \n",
    "        with open(updated_tsv_name, 'w') as outfile:\n",
    "            # get keys from OrderedDictionary and write rows, separate with tabs\n",
    "            writer = csv.DictWriter(outfile, tsv_headers, delimiter=\"\\t\")\n",
    "            writer.writeheader()\n",
    "            writer.writerows(drs_dict_table)\n",
    "    \n",
    "    print(f'Saving DRS URL updated {etype} TSV to {ws_bucket}')\n",
    "    !gsutil cp $updated_tsv_name $ws_bucket 2> stdout\n",
    "    \n",
    "    modified_cols = list(set(modified_cols))\n",
    "    if dry_run:\n",
    "        print(f'The following columns in the {etype} table *will be* be updated:')\n",
    "        if not modified_cols:\n",
    "            print('\\t' * 4 + f\"No columns to update in the {etype} table.\" + \"\\n\\n\")\n",
    "        else:\n",
    "            print('\\n'.join(['\\t' * 4 + c for c in modified_cols]))\n",
    "            print(f'To view what will be updated, inspect the {updated_tsv_name} file in the workspace bucket, {ws_bucket}.' + \"\\n\\n\")\n",
    "    else:\n",
    "        # upload newly created tsv file containing drs urls\n",
    "        response = fapi.upload_entities_tsv(ws_project, ws_name, updated_tsv_name, model=\"flexible\")\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Could not update existing {etype} table. Error message: {response.text}\")\n",
    "        \n",
    "        print(f'Finished uploading TCGA DRS updated .tsv for entity: {etype}' + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
